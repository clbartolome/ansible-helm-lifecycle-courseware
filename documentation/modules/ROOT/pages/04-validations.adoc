= Validations

Now that the namespace has been created and the test *base-chart* has been installed, we can start performing some tests to be sure the *base-chart* is working as expected.

The first thing that we should do is create a task to download the *base-chart* code and read the scenario1 values so we can perform some validations.

[tabs, subs="attributes+,+macros"]	
====	
Steps::	
+	
--	
* Go to *Gitea* and open *helm-ansible-pipeline* repository.
* Open the playbook *02-validate-test-scenario.yml* and edit it.
* Add a task to clone *base-chart* repository (you can use /tmp/basechart as destination).
* Add a task to read the values of the scenario1 in */tmp/base-chart/test/scenario1/values.yaml* as variables.
* Commit your changes.

TIP: Use the collection *https://docs.ansible.com/ansible/latest/collections/ansible/builtin/git_module.html[ansible.builtin.git]* to clone the repository.

TIP: Use the collection *https://docs.ansible.com/ansible/latest/collections/ansible/builtin/include_vars_module.html[ansible.builtin.include_vars]* read variables from a *yaml* file.

--	
Solution::	
+	
--	
* Go to *Gitea* and open *helm-ansible-pipeline* repository.
* Open the playbook *02-validate-test-scenario.yml* and edit it.
* Add a task to clone *base-chart* repository (#replace <student> with your student id#):

[.console-input]
[source,yml,subs="attributes+,+macros"]	
----	
- name: Clone Base Chart remote repository
  ansible.builtin.git:
    repo: http://gitea.app-lifecycle-demo.svc:3000/<student>/base-chart.git
    dest: /tmp/base-chart
    version: master
----	

* Add a task to read the values of the scenario1 in */tmp/base-chart/test/scenario1/values.yaml* as variables:

[.console-input]
[source,yml,subs="attributes+,+macros"]	
----	
- name: Read scenario values
  ansible.builtin.include_vars:
    file: "/tmp/base-chart/test/scenario1/values.yaml"
    name: scenario
----	

* Commit your changes.
====

[#replicas]
== Replicas

In this test we're going to validate if the number of replicas we defined in our scenario1 is the same as the number of replicas that we have in the *base-chart* that we've installed.

To achive that we're going to read the deployment object and check the running replicas.

[tabs, subs="attributes+,+macros"]	
====	
Steps::	
+	
--	
* Go to *Gitea* and open *helm-ansible-pipeline* repository.
* Open the playbook *02-validate-test-scenario.yml* and edit it.
* Add a task to get the *deployment* that it is running in your test namespace.
* Add a task to compare the running *deployment* replicas with the defined in the scenario1 values (set the already defined variable *failed* as true if it is different).
* Commit your changes.

TIP: Use the collection *https://docs.ansible.com/ansible/latest/collections/kubernetes/core/k8s_module.html[kubernetes.core.k8s]* to get the deployment.

--	
Solution::	
+	
--	
* Go to *Gitea* and open *helm-ansible-pipeline* repository.
* Open the playbook *02-validate-test-scenario.yml* and edit it.
* Add a task to get the *deployment* that it is running in your test namespace (#replace <student> with your student id#):

[.console-input]
[source,yml,subs="attributes+,+macros"]	
----	
- name: Test - Deployment --> Get deployment
  kubernetes.core.k8s_info:
    api_version: apps/v1
    kind: Deployment
    namespace: "<student>-base-chart"
    label_selectors:
      - app={{ helm_test_scenario }}
  ignore_errors: true
  register: scenario_deployment
----	

* Add a task to compare the running *deployment* replicas with the defined in the scenario1 values (set the already defined variable *failed* as true if it is different):

[.console-input]
[source,yml,subs="attributes+,+macros"]	
----	
- name: Test - Deployment --> Validate the number of replicas is correct
  ansible.builtin.set_fact:
    failed: true
  when: scenario_deployment.resources[0].spec.replicas != scenario.deploy.replicas
----	

* Commit your changes.
====

[#route]
== Route

In the scenario1 we've enabled the creatrion of a route with *deploy.route.enabled: true*, so we should validate if a route has been created.

To achive that we have different options, we could read the route object and check if exists but we're going one step forward and also validate if we can connect to our application through the route.

[tabs, subs="attributes+,+macros"]	
====	
Steps::	
+	
--	
* Go to *Gitea* and open *helm-ansible-pipeline* repository.
* Open the playbook *02-validate-test-scenario.yml* and edit it.
* Add a task to get the *route* that should exist in your test namespace.
* Add a task to send a request to the route, get the url from the route we've get on previous step.
* Add a task to validate the previous step response code is 200 (set the already defined variable *failed* as true if it is different).
* Commit your changes.

TIP: Use the collection *https://docs.ansible.com/ansible/latest/collections/kubernetes/core/k8s_module.html[kubernetes.core.k8s]* to get the route.

TIP: Use the collection *https://docs.ansible.com/ansible/latest/collections/ansible/builtin/uri_module.html[ansible.builtin.uri]* to send the request.

CAUTION: Ignore errors while getting the route, the route creation could be disabled in future scenarios...

--	
Solution::	
+	
--	
* Go to *Gitea* and open *helm-ansible-pipeline* repository.
* Open the playbook *02-validate-test-scenario.yml* and edit it.
* Add a task to get the *route* that should exist in your test namespace (#replace <student> with your student id#):

[.console-input]
[source,yml,subs="attributes+,+macros"]	
----	
- name: Test - Route --> Get route
  kubernetes.core.k8s_info:
    api_version: route.openshift.io/v1
    kind: Route
    namespace: "<student>-base-chart"
    label_selectors:
      - app=scenario1
  register: scenario_route
  ignore_errors: true
----	

* Add a task to send a request to the route, get the url from the route we've get on previous step:

[.console-input]
[source,yml,subs="attributes+,+macros"]	
----	
- name: Test - Route --> Send request to the route when route is defined and enabled
  ansible.builtin.uri:
    url: "http://{{ scenario_route.resources[0].spec.host }}"
    method: GET
    status_code: 200
    return_content: yes       
  ignore_errors: true  
  register: route_response   
  when: scenario.deploy.route is defined and scenario.deploy.route.enabled == true and scenario_route.resources | length > 0
----	

* Add a task to validate the previous step response code is *200* (set the already defined variable *failed* as true if it is different):

[.console-input]
[source,yml,subs="attributes+,+macros"]	
----	
- name: Test - Route --> Validate route is working when is defined and enabled
  ansible.builtin.set_fact:
    failed: true
  when: scenario.deploy.route is defined and scenario.deploy.route.enabled == true and scenario_route.resources | length > 0 and route_response.status != 200
----	

* Commit your changes.
====

[#size]
== Size

To simplify application resources (memory and CPU) the *base-chart* will preconfigure a set of sizes (XS, S, M, L,...) so application teams don't have to worry about anything else but to choose one. On the operation teams side, that will allow them to manage and track the resources utlization (a label with the size is included in all chart resources).

Currently the *base-chart* only includes the size *S* and that is the one we're going to valide in this scenario. In *Gitea* > *base-chart/chart/template/_helpers.tpl* is where the size is implemented:

[source,yml,subs="attributes+,+macros"]	
----	
{{- define "base.resources" -}}
{{- $size := default "S" .Values.size -}}
resources:
{{- if eq $size "S" }}
  limits:
    cpu: 100m
    memory: 256Mi
  requests:
    cpu: 100m
    memory: 256Mi
{{- else }}
  limits:
    cpu: 100m
    memory: 256Mi
  requests:
    cpu: 100m
    memory: 256Mi
{{- end}}
{{- end -}}
----	

As you can see the expected resources for *S* are:

[cols="^,^", options="header"]
|===
|Property |Value

|request.cpu
|100m
|request.memory
|256Mi
|limit.cpu
|100m
|limit.memory
|256Mi

|===

The scenario1 is configured with the size *S* so we have to validate that the installed deployment have the expected configuration for both limit and request.

[tabs, subs="attributes+,+macros"]	
====	
Steps::	
+	
--	
* Go to *Gitea* and open *helm-ansible-pipeline* repository.
* Open the playbook *02-validate-test-scenario.yml* and edit it.
* Add a task to validate that the *S* size resources are the same as the installed ones (set the already defined variable *failed* as true if it is different).
* Commit your changes.

NOTE: We don't need to get the deployment because we already got it in the replicas validation.

TIP: Remember that *S* is also the default size.

--	
Solution::	
+	
--	
* Go to *Gitea* and open *helm-ansible-pipeline* repository.
* Open the playbook *02-validate-test-scenario.yml* and edit it.
* Add a task to validate that the *S* size resources are the same as the installed ones (set the already defined variable *failed* as true if it is different):

[.console-input]
[source,yml,subs="attributes+,+macros"]	
----	
- name: Test - Application Size - S or default
  ansible.builtin.set_fact:
    failed: true
  when: (scenario.size is undefined or scenario.size == "S") and not
        (scenario_deployment.resources[0].spec.template.spec.containers[0].resources.limits.cpu == "100m" and
        scenario_deployment.resources[0].spec.template.spec.containers[0].resources.limits.memory == "256Mi" and
        scenario_deployment.resources[0].spec.template.spec.containers[0].resources.requests.cpu == "100m" and
        scenario_deployment.resources[0].spec.template.spec.containers[0].resources.requests.memory == "256Mi")
----	


* Commit your changes.
====      

        

        

        





          